{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homelab Airflow DAGs","text":""},{"location":"#overview","title":"\ud83d\ude80 Overview","text":"<p>Homelab Airflow DAGs is a dynamic DAG management system for Apache Airflow that enables you to define and manage your data pipelines using simple YAML configuration files. Perfect for homelab environments where you need flexible, maintainable, and version-controlled workflow automation.</p>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"#using-pip","title":"Using pip","text":"<pre><code>pip install homelab_airflow_dags\n</code></pre>"},{"location":"#using-poetry","title":"Using Poetry","text":"<pre><code>poetry add homelab_airflow_dags\n</code></pre>"},{"location":"#using-uv","title":"Using uv","text":"<pre><code>uv add homelab_airflow_dags\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>Apache Airflow 2.0 or higher</li> <li>Access to a Python package index (PyPI or private)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<p>The simplest way to install homelab-airflow-dags:</p> <pre><code>pip install homelab_airflow_dags\n</code></pre>"},{"location":"getting-started/installation/#using-poetry","title":"Using Poetry","text":"<p>If you're using Poetry for dependency management:</p> <pre><code>poetry add homelab_airflow_dags\n</code></pre>"},{"location":"getting-started/installation/#using-uv","title":"Using uv","text":"<p>For faster installation with uv:</p> <pre><code>uv add homelab_airflow_dags\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development or contributing to the project:</p> <pre><code>git clone https://github.com/ShawnDen-coder/homelab_airflow_dags.git\ncd homelab_airflow_dags\npip install -e .\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>homelab_airflow_dags<ul> <li>common_tasks<ul> <li>oss_operator</li> <li>risk_management_task</li> </ul> </li> <li>config</li> <li>constants</li> <li>dags<ul> <li>load_dag</li> <li>risk_portfolio</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/homelab_airflow_dags/","title":"Homelab Airflow Dags","text":""},{"location":"reference/homelab_airflow_dags/config/","title":"Config","text":"<p>Configuration management module for Consul-based settings.</p> <p>This module provides a centralized configuration management system using HashiCorp Consul as the backend storage. It supports dynamic configuration loading with caching mechanisms and hot-reload capabilities for environment variables.</p> <p>The module automatically constructs configuration keys using the pattern: <code>{CONFIG_PREFIX}/{PROJECT_NAME}/{config_name}</code> where CONFIG_PREFIX defaults to 'cfg' and PROJECT_NAME defaults to 'homelab-airflow-dags'.</p> Environment Variables <p>PROJECT_NAME: Name of the project (default: 'homelab-airflow-dags') CONFIG_PREFIX: Prefix for configuration keys (default: 'cfg') CONSUL_HOST: Consul server hostname (default: 'localhost') CONSUL_PORT: Consul server port (default: '8500') CONSUL_TOKEN: Consul authentication token (optional)</p> Example <p>Basic usage for getting and setting configurations:</p> <pre><code>from homelab_airflow_dags.config import get_config, set_config\n\n# Get configuration with default fallback\ndb_config = get_config(\"database_config\", {\"host\": \"localhost\"})\n</code></pre> Note <p>The Consul client is cached using LRU cache with maxsize=1 to optimize performance while still supporting hot-reload when environment variables change.</p>"},{"location":"reference/homelab_airflow_dags/config/#homelab_airflow_dags.config-functions","title":"Functions","text":""},{"location":"reference/homelab_airflow_dags/config/#homelab_airflow_dags.config.get_config","title":"get_config","text":"<pre><code>get_config(config_name: str, default: Any = None) -&gt; Any\n</code></pre> <p>Retrieve configuration from Consul with YAML parsing.</p> <p>Fetches configuration data from Consul using the specified config name, automatically parsing YAML content and returning the parsed object. If the configuration doesn't exist or parsing fails, returns the default value.</p> <p>Parameters:</p> Name Type Description Default <code>config_name</code> <code>str</code> <p>The name of the configuration to retrieve.</p> required <code>default</code> <code>Any</code> <p>Default value to return if config is not found or parsing fails.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The parsed configuration object, or default value if not found/invalid.</p> Example <pre><code>config = get_config(\"api_settings\", {\"timeout\": 30})\nprint(config.get(\"timeout\"))  # Output: 30\n</code></pre>"},{"location":"reference/homelab_airflow_dags/config/#homelab_airflow_dags.config.get_consul_client","title":"get_consul_client","text":"<pre><code>get_consul_client()\n</code></pre> <p>Get a Consul client with hot-reload support for environment variables.</p> <p>This function retrieves a Consul client that automatically updates when environment variables change. It uses LRU caching to optimize performance while maintaining the ability to pick up configuration changes at runtime.</p> <p>Returns:</p> Type Description <p>consul.Consul: A configured Consul client instance.</p> Environment Variables <p>CONSUL_HOST: Consul server hostname (default: 'localhost') CONSUL_PORT: Consul server port (default: '8500') CONSUL_TOKEN: Consul authentication token (default: '')</p>"},{"location":"reference/homelab_airflow_dags/constants/","title":"Constants","text":""},{"location":"reference/homelab_airflow_dags/common_tasks/","title":"Common Tasks","text":""},{"location":"reference/homelab_airflow_dags/common_tasks/oss_operator/","title":"Oss Operator","text":"<p>MinIO Upload Task Module.</p> <p>Simple MinIO upload functionality using Consul configuration. Supports both file-based and memory-based uploads.</p>"},{"location":"reference/homelab_airflow_dags/common_tasks/oss_operator/#homelab_airflow_dags.common_tasks.oss_operator-functions","title":"Functions","text":""},{"location":"reference/homelab_airflow_dags/common_tasks/oss_operator/#homelab_airflow_dags.common_tasks.oss_operator.get_content_type","title":"get_content_type","text":"<pre><code>get_content_type(file_path: str) -&gt; str\n</code></pre> <p>Determine the MIME type of a file based on its file extension.</p> <p>This function uses Python's built-in mimetypes module to guess the MIME type of a file based on its extension. It provides a fallback to a generic binary MIME type for files with unknown or unrecognized extensions.</p> <p>The function is essential for proper file handling in object storage systems like MinIO, as it ensures that files are served with the correct Content-Type header, enabling proper browser handling and preview functionality.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file for which to determine the MIME type.       Only the file extension is used for type detection, so the       file doesn't need to exist at the specified path.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The MIME type string corresponding to the file extension.      Returns \"application/octet-stream\" for unknown file types.</p> Example <p>Common file type detection:</p> <pre><code># Image files\nassert get_content_type(\"image.jpg\") == \"image/jpeg\"\nassert get_content_type(\"photo.png\") == \"image/png\"\n\n# Document files\nassert get_content_type(\"document.pdf\") == \"application/pdf\"\nxlsx_mime = \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\nassert get_content_type(\"spreadsheet.xlsx\") == xlsx_mime\n\n# Text files\nassert get_content_type(\"data.csv\") == \"text/csv\"\nassert get_content_type(\"config.json\") == \"application/json\"\n\n# Unknown extensions\nassert get_content_type(\"file.unknown\") == \"application/octet-stream\"\n</code></pre> Note <ul> <li>The function initializes the mimetypes module if not already done</li> <li>Only the file extension is considered, not the actual file content</li> <li>The detection is case-insensitive for file extensions</li> <li>Binary fallback ensures compatibility with all file types</li> </ul>"},{"location":"reference/homelab_airflow_dags/common_tasks/oss_operator/#homelab_airflow_dags.common_tasks.oss_operator.get_minio_s3_hook","title":"get_minio_s3_hook","text":"<pre><code>get_minio_s3_hook(config_name: str = 'minio_auth_config') -&gt; S3Hook\n</code></pre> <p>Create S3Hook using MinIO configuration from Consul.</p> <p>This function creates an S3Hook by setting up environment variables for the AWS connection, which is a simpler approach that doesn't require database operations.</p> <p>Parameters:</p> Name Type Description Default <code>config_name</code> <code>str</code> <p>Name of the configuration in Consul</p> <code>'minio_auth_config'</code> <p>Returns:</p> Name Type Description <code>S3Hook</code> <code>S3Hook</code> <p>Configured S3Hook instance</p> Example Consul Configuration <pre><code>minio_endpoint: \"http://minio.minio.svc.cluster.local:9000\"\naccess_key: \"your-minio-access-key\"\nsecret_key: \"your-minio-secret-key\"\nregion: \"us-east-1\"\n</code></pre>"},{"location":"reference/homelab_airflow_dags/common_tasks/oss_operator/#homelab_airflow_dags.common_tasks.oss_operator.upload_file","title":"upload_file","text":"<pre><code>upload_file(file_path: str, object_key: str, bucket_name: str = 'airflow', config_name: str = 'minio_auth_config') -&gt; bool\n</code></pre> <p>Upload a file to MinIO object storage with automatic MIME type detection.</p> <p>This function provides the core file upload functionality to MinIO object storage. It handles file validation, MIME type detection, S3Hook configuration, and the actual upload operation with proper error handling and logging.</p> <p>The function automatically detects the file's MIME type based on its extension and sets appropriate metadata for proper file handling in MinIO. It uses the S3Hook from Airflow's Amazon provider to ensure compatibility with S3-compatible storage systems like MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The local file system path to the file that needs to be uploaded.       Must point to an existing file with read permissions.</p> required <code>object_key</code> <code>str</code> <p>The target object key/path within the MinIO bucket. This serves        as the unique identifier for the file within the bucket.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the target MinIO bucket. The bucket must exist         before attempting the upload. Defaults to \"airflow\".</p> <code>'airflow'</code> <code>config_name</code> <code>str</code> <p>The name of the configuration key in Consul that contains         MinIO authentication details. Defaults to \"minio_auth_config\".</p> <code>'minio_auth_config'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file upload completed successfully, False if the upload       failed due to any reason (file not found, connection issues, etc.).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the MinIO configuration is not found in Consul or if required        configuration fields are missing from the Consul configuration.</p> <code>FileNotFoundError</code> <p>If the specified local file does not exist at the given               path or if there are permission issues reading the file.</p> <code>ConnectionError</code> <p>If unable to establish connection to MinIO service or if             authentication fails with the provided credentials.</p> <code>S3UploadFailedError</code> <p>If the upload operation fails due to network issues,                 storage quota exceeded, or other MinIO service errors.</p> Example <p>Basic file upload:</p> <pre><code>success = upload_file(\n    file_path=\"/tmp/data.pkl\",\n    object_key=\"data/processed/result.pkl\",\n    bucket_name=\"analytics\"\n)\nif success:\n    print(\"File uploaded successfully\")\n</code></pre> <p>Upload with custom configuration:</p> <pre><code>success = upload_file(\n    file_path=\"/home/user/report.pdf\",\n    object_key=\"reports/monthly/report.pdf\",\n    bucket_name=\"documents\",\n    config_name=\"custom_minio_config\"\n)\n</code></pre> Note <ul> <li>The function automatically sets Content-Type based on file extension</li> <li>Cache-Control header is set to \"max-age=3600\" for better performance</li> <li>All upload operations use the \"replace=True\" option to overwrite existing files</li> <li>Detailed logging is provided for both success and failure cases</li> </ul>"},{"location":"reference/homelab_airflow_dags/common_tasks/oss_operator/#homelab_airflow_dags.common_tasks.oss_operator.upload_minio","title":"upload_minio","text":"<pre><code>upload_minio(file_path: str, object_key: str, bucket_name: str = 'airflow') -&gt; bool\n</code></pre> <p>Upload a local file to MinIO object storage with automatic retry and MIME type detection.</p> <p>This Airflow task provides a robust file upload mechanism to MinIO object storage with built-in retry logic, automatic MIME type detection, and comprehensive error handling. The task integrates with Consul for dynamic configuration management and uses the S3Hook for reliable file transfer operations.</p> <p>The task performs the following operations: 1. Validates the existence of the local file 2. Retrieves MinIO authentication configuration from Consul 3. Detects the file's MIME type based on its extension 4. Creates an S3Hook connection to MinIO 5. Uploads the file with proper metadata 6. Logs the upload results for monitoring</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The local file system path to the file that needs to be uploaded.       Must be an absolute or relative path to an existing file.</p> required <code>object_key</code> <code>str</code> <p>The target object key/path within the MinIO bucket where the file        will be stored. This acts as the file's identifier in the bucket.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the target MinIO bucket. Defaults to \"airflow\".         The bucket must exist before attempting the upload.</p> <code>'airflow'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file upload was successful, False if the upload failed       due to file not found, connection issues, or other errors.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the MinIO authentication configuration is not found in Consul        or if required configuration fields are missing.</p> <code>FileNotFoundError</code> <p>If the specified local file does not exist at the given path.</p> <code>ConnectionError</code> <p>If unable to establish connection to MinIO service or if             authentication fails with the provided credentials.</p> <code>PermissionError</code> <p>If insufficient permissions to read the local file or write             to the specified MinIO bucket.</p> <code>S3UploadFailedError</code> <p>If the S3/MinIO upload operation fails due to network                 issues, storage quota, or other service-related problems.</p> Note <p>This task is designed for use in Airflow DAGs and requires: - Active Consul service with proper MinIO configuration - Valid \"minio_auth_config\" in Consul containing:   * minio_endpoint: MinIO service endpoint URL   * access_key: MinIO access key for authentication   * secret_key: MinIO secret key for authentication   * region: Storage region (optional, defaults to \"us-east-1\") - Network connectivity to both Consul and MinIO services - Proper file system permissions for reading the source file</p> Example <p>Basic usage in an Airflow DAG:</p> <pre><code>from homelab_airflow_dags.common_tasks.oss_operator import upload_minio\n\n@dag(schedule=\"@daily\")\ndef my_data_pipeline():\n    # Upload processed data file\n    upload_task = upload_minio(\n        file_path=\"/tmp/processed_data.csv\",\n        object_key=\"data/daily/processed_data.csv\",\n        bucket_name=\"data-lake\"\n    )\n</code></pre> <p>Advanced usage with task dependencies:</p> <pre><code>@dag(schedule=\"@daily\")\ndef advanced_pipeline():\n    # Process data first\n    process_task = process_data()\n\n    # Then upload the result\n    upload_task = upload_minio(\n        file_path=\"{{ ti.xcom_pull(task_ids='process_data')['output_file'] }}\",\n        object_key=\"results/{{ ds }}/output.pkl\",\n        bucket_name=\"analytics\"\n    )\n\n    process_task &gt;&gt; upload_task\n</code></pre> <p>Consul configuration example for \"minio_auth_config\":</p> <pre><code>{\n    \"minio_endpoint\": \"http://minio.homelab.local:9000\",\n    \"access_key\": \"minioadmin\",\n    \"secret_key\": \"minioadmin123\",\n    \"region\": \"us-east-1\"\n}\n</code></pre> See Also <p>upload_file: The underlying function that performs the actual upload operation. get_minio_s3_hook: Function that creates the S3Hook with MinIO configuration. get_content_type: Function that determines the MIME type of files.</p>"},{"location":"reference/homelab_airflow_dags/common_tasks/risk_management_task/","title":"Risk Management Task","text":"<p>Portfolio Risk Management Task Module.</p> <p>This module provides Airflow task functions for portfolio risk management analysis and optimization. It integrates with the ibkr_quant library to perform comprehensive portfolio optimization using modern portfolio theory and generates formatted risk analysis reports.</p> <p>The module is designed to work with Consul for configuration management, allowing dynamic parameter updates without code changes. It serves as the core execution engine for portfolio risk management workflows in Airflow.</p> Example <p>Import and use the task in an Airflow DAG:</p> <pre><code>from homelab_airflow_dags.common_tasks.risk_management_task import risk_management_task\n\n@dag(schedule=\"@daily\")\ndef my_risk_dag():\n    risk_management_task()\n</code></pre> Note <p>This module requires: - Consul configuration service with \"risk_portfolio_config\" key - ibkr_quant library for risk management functionality - Proper Airflow task environment setup</p>"},{"location":"reference/homelab_airflow_dags/common_tasks/risk_management_task/#homelab_airflow_dags.common_tasks.risk_management_task-functions","title":"Functions","text":""},{"location":"reference/homelab_airflow_dags/common_tasks/risk_management_task/#homelab_airflow_dags.common_tasks.risk_management_task.risk_management_task","title":"risk_management_task","text":"<pre><code>risk_management_task() -&gt; dict\n</code></pre> <p>Execute comprehensive portfolio risk management analysis and optimization.</p> <p>This Airflow task performs end-to-end portfolio risk management analysis using modern portfolio theory and optimization algorithms. It loads configuration from Consul, builds a portfolio optimization strategy, executes the optimization algorithms, and returns detailed portfolio analysis results.</p> <p>The task integrates with the ibkr_quant library's StrategyBuilder to perform: - Portfolio composition analysis and optimization - Risk metrics calculation (VaR, CVaR, Sharpe ratio, etc.) - Expected returns and volatility analysis - Efficient frontier computation - Portfolio performance attribution</p> <p>The function is designed to be configuration-driven, allowing portfolio parameters to be dynamically updated through Consul without code changes. This enables flexible portfolio management workflows and easy parameter tuning for different market conditions.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A comprehensive dictionary containing portfolio optimization results     and risk analysis metrics. The dictionary includes:     - portfolio_weights: Optimized asset allocation weights     - expected_returns: Expected return for each asset     - risk_metrics: Calculated risk measures (VaR, CVaR, volatility)     - performance_metrics: Portfolio performance indicators     - optimization_details: Technical optimization results     - market_data: Underlying market data used in analysis</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Consul configuration \"risk_portfolio_config\" is not found or contains invalid parameters that cannot be validated by StrategyConfig.</p> <code>ConfigurationError</code> <p>If the strategy configuration fails validation due to missing required fields or invalid parameter combinations.</p> <code>OptimizationError</code> <p>If the portfolio optimization algorithms fail to converge or encounter numerical issues during execution.</p> <code>DataError</code> <p>If required market data is unavailable or contains insufficient historical data for reliable optimization.</p> <code>ConnectionError</code> <p>If unable to connect to external data sources or services required for portfolio analysis.</p> Note <p>This task requires the following external dependencies and configurations: - Active Consul service with \"risk_portfolio_config\" key containing:   * asset_symbols: List of ticker symbols for portfolio assets   * optimization_method: Portfolio optimization algorithm to use   * risk_parameters: Risk management configuration parameters   * data_source_config: Market data source configuration - Network connectivity to market data providers - Sufficient computational resources for optimization algorithms - Valid API credentials for data sources (if required)</p> Example <p>Basic usage in an Airflow DAG:</p> <pre><code>from homelab_airflow_dags.common_tasks.risk_management_task import risk_management_task\n\n@dag(schedule=\"@daily\")\ndef portfolio_optimization_dag():\n    # Execute daily portfolio optimization\n    results = risk_management_task()\n\n    # Use results in downstream tasks\n    generate_report(results)\n</code></pre> <p>Consul configuration example for \"risk_portfolio_config\":</p> <pre><code>{\n    \"asset_symbols\": [\"AAPL\", \"GOOGL\", \"MSFT\", \"TSLA\"],\n    \"optimization_method\": \"max_sharpe\",\n    \"risk_free_rate\": 0.02,\n    \"lookback_period\": 252,\n    \"data_source\": \"yahoo_finance\",\n    \"risk_parameters\": {\n        \"max_volatility\": 0.20,\n        \"min_weight\": 0.05,\n        \"max_weight\": 0.40\n    }\n}\n</code></pre> See Also <p>StrategyBuilder: The core portfolio optimization engine from ibkr_quant. StrategyConfig: Configuration validation and management class. get_config: Consul configuration retrieval function.</p>"},{"location":"reference/homelab_airflow_dags/dags/","title":"Dags","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/","title":"Load Dag","text":"<p>Dynamic DAG loader for Apache Airflow using YAML configuration files.</p> <p>This module provides functionality to dynamically load and generate Apache Airflow DAGs from YAML configuration files using the dagfactory library. It searches for YAML files in configured directories and automatically generates DAGs based on their definitions.</p> <p>The module supports multiple configuration directories through environment variables and provides a default fallback to the standard Airflow DAGs folder.</p> Environment Variables <p>CONFIG_ROOT_DIRS: Colon-separated list of directories containing DAG YAML files.                   Example: \"/path/to/dags:/another/path/to/dags\" AIRFLOW_PROJ_DIR: Base directory for the Airflow project (default: /opt/airflow).</p> Typical usage example Note <p>This module automatically executes the main() function when imported, which will load all DAGs from YAML files found in the configured directories.</p>"},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--this-module-is-typically-imported-by-airflow-to-automatically-load-dags","title":"This module is typically imported by Airflow to automatically load DAGs","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--the-main-function-is-executed-when-the-module-is-imported","title":"The main() function is executed when the module is imported","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--yaml-files-should-be-placed-in","title":"YAML files should be placed in:","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--1-directories-specified-in-config_root_dirs-environment-variable","title":"1. Directories specified in CONFIG_ROOT_DIRS environment variable","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--2-default-location-airflow_proj_dirdags","title":"2. Default location: $AIRFLOW_PROJ_DIR/dags/","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--example-yaml-structure","title":"Example YAML structure:","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--my_dag","title":"my_dag:","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--default_args","title":"default_args:","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--owner-airflow","title":"owner: 'airflow'","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--start_date-2024-01-01","title":"start_date: '2024-01-01'","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--schedule_interval-daily","title":"schedule_interval: '@daily'","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--tasks","title":"tasks:","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--task_1","title":"task_1:","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--operator-airflowoperatorsbashbashoperator","title":"operator: airflow.operators.bash.BashOperator","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag--bash_command-echo-hello-world","title":"bash_command: 'echo \"Hello World\"'","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag-functions","title":"Functions","text":""},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag.find_yamls","title":"find_yamls","text":"<pre><code>find_yamls(config_root_dir: str) -&gt; list[str]\n</code></pre> <p>Find all YAML files in the specified directories.</p> <p>Parameters:</p> Name Type Description Default <code>config_root_dir</code> <code>str</code> <p>A colon-separated string of directories to search for YAML files.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A sorted list of paths to YAML files found in the specified directories.</p>"},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag.load_dag_floders","title":"load_dag_floders","text":"<pre><code>load_dag_floders() -&gt; list[str]\n</code></pre> <p>Load directories containing DAG YAML files.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of directories containing YAML files.</p>"},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag.load_dags","title":"load_dags","text":"<pre><code>load_dags(yaml_files: list[str]) -&gt; None\n</code></pre> <p>Load DAGs from the specified YAML files.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_files</code> <code>list[str]</code> <p>A list of paths to YAML files containing DAG definitions.</p> required"},{"location":"reference/homelab_airflow_dags/dags/load_dag/#homelab_airflow_dags.dags.load_dag.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Main function to load DAGs from YAML files.</p>"},{"location":"reference/homelab_airflow_dags/dags/risk_portfolio/","title":"Risk Portfolio","text":"<p>Portfolio Risk Management DAG Module.</p> <p>This module defines an Airflow DAG for daily portfolio risk management analysis and optimization using the ibkr_quant library. The DAG performs comprehensive risk analysis, portfolio optimization, and generates formatted reports.</p> <p>The DAG is designed to run daily and integrates with Consul for configuration management and the ibkr_quant library for risk management functionality.</p> Example <p>The DAG can be triggered manually or runs automatically on a daily schedule:</p> <pre><code># Manual trigger via Airflow CLI\nairflow dags trigger risk_management\n</code></pre> <p>Attributes:</p> Name Type Description <code>risk_management_dag</code> <p>The instantiated DAG object for Airflow to discover.</p> Note <p>This module requires: - Consul configuration service with \"risk_portfolio_config\" key - ibkr_quant library for risk management functionality - Proper Airflow environment setup</p>"},{"location":"reference/homelab_airflow_dags/dags/risk_portfolio/#homelab_airflow_dags.dags.risk_portfolio-functions","title":"Functions","text":""},{"location":"reference/homelab_airflow_dags/dags/risk_portfolio/#homelab_airflow_dags.dags.risk_portfolio.risk_management","title":"risk_management","text":"<pre><code>risk_management()\n</code></pre> <p>Define the portfolio risk management DAG with data persistence.</p> <p>This function creates an Airflow DAG that performs daily portfolio risk management analysis and optimization, followed by data serialization and upload to MinIO object storage. The DAG executes a comprehensive risk analysis workflow that includes portfolio optimization, risk metric calculation, result serialization, and persistent storage.</p> <p>The DAG is configured to run daily and uses Consul for configuration management, allowing for dynamic parameter updates without code changes. It integrates with the ibkr_quant library to perform sophisticated portfolio optimization using modern portfolio theory, and stores results in MinIO for long-term persistence and analysis.</p> <p>Returns:</p> Name Type Description <code>None</code> <p>This function defines the DAG structure and task dependencies.       The actual DAG object is created by the @dag decorator.</p> Note <p>This function is decorated with @dag and serves as the DAG definition. The actual execution logic is implemented in the task functions.</p> <p>Key features: - Daily execution schedule - Single active run limitation - No historical backfill - Consul-based configuration management - Comprehensive risk analysis and optimization - Automatic data persistence to MinIO object storage</p> Example <p>The DAG structure created by this function:</p> <pre><code>risk_management_dag\n\u251c\u2500\u2500 risk_management_task (generates PortfolioInfo)\n\u251c\u2500\u2500 serialize_portfolio (serializes to file and returns paths)\n\u2514\u2500\u2500 upload_minio (uploads file to MinIO storage)\n</code></pre> See Also <p>risk_management_task: The main task that performs the risk analysis. serialize_and_upload_portfolio: Task that serializes and uploads results. StrategyBuilder: The portfolio optimization engine from ibkr_quant. PortfolioDisplay: The report formatting utility from ibkr_quant.</p>"}]}