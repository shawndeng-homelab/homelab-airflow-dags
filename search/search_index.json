{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homelab Airflow DAGs","text":""},{"location":"#overview","title":"\ud83d\ude80 Overview","text":"<p>Homelab Airflow DAGs is a dynamic DAG management system for Apache Airflow that enables you to define and manage your data pipelines using simple YAML configuration files. Perfect for homelab environments where you need flexible, maintainable, and version-controlled workflow automation.</p>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"#using-pip","title":"Using pip","text":"<pre><code>pip install homelab_airflow_dags\n</code></pre>"},{"location":"#using-poetry","title":"Using Poetry","text":"<pre><code>poetry add homelab_airflow_dags\n</code></pre>"},{"location":"#using-uv","title":"Using uv","text":"<pre><code>uv add homelab_airflow_dags\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>Apache Airflow 2.0 or higher</li> <li>Access to a Python package index (PyPI or private)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<p>The simplest way to install homelab-airflow-dags:</p> <pre><code>pip install homelab_airflow_dags\n</code></pre>"},{"location":"getting-started/installation/#using-poetry","title":"Using Poetry","text":"<p>If you're using Poetry for dependency management:</p> <pre><code>poetry add homelab_airflow_dags\n</code></pre>"},{"location":"getting-started/installation/#using-uv","title":"Using uv","text":"<p>For faster installation with uv:</p> <pre><code>uv add homelab_airflow_dags\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development or contributing to the project:</p> <pre><code>git clone https://github.com/ShawnDen-coder/homelab_airflow_dags.git\ncd homelab_airflow_dags\npip install -e .\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>homelab_airflow_dags<ul> <li>common_tasks<ul> <li>oss_operator</li> </ul> </li> <li>config</li> <li>constants</li> <li>dags<ul> <li>ibkr_account_snapshot</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/homelab_airflow_dags/","title":"Homelab Airflow Dags","text":""},{"location":"reference/homelab_airflow_dags/config/","title":"Config","text":"<p>Configuration manager for Airflow DAGs using Consul key-value store.</p> <p>This module provides a high-level interface for accessing centralized configurations stored in Consul. It automatically handles environment-specific settings, key namespacing, and data type conversions. The module is designed to work seamlessly with the Homelab configuration system.</p> The configuration path in Consul follows this pattern <p>cfg// <p>For example, if your package name is \"homelab-airflow-dags\" and you request the key \"database/mysql\", it will look for:     cfg/homelab-airflow-dags/database/mysql</p> Environment Variables <p>HOMELAB_ENVIRONMENT: Runtime environment (dev/staging/prod). HOMELAB_CONSUL_URLS: Comma-separated list of Consul server URLs. HOMELAB_CONSUL_TOKEN: Consul ACL Token for authentication.</p> Example <p>from homelab_airflow_dags.config import get_config</p>"},{"location":"reference/homelab_airflow_dags/config/#homelab_airflow_dags.config--get-risk-portfolio-settings","title":"Get risk portfolio settings","text":"<p>risk_config = get_config(\"risk_portfolio_config\") if risk_config: ...     threshold = risk_config.get(\"risk_threshold\") ...     window_size = risk_config.get(\"window_size\")</p>"},{"location":"reference/homelab_airflow_dags/config/#homelab_airflow_dags.config--get-data-source-configuration","title":"Get data source configuration","text":"<p>source_config = get_config(\"data_source/market\")</p>"},{"location":"reference/homelab_airflow_dags/config/#homelab_airflow_dags.config-functions","title":"Functions","text":""},{"location":"reference/homelab_airflow_dags/config/#homelab_airflow_dags.config.get_config","title":"get_config","text":"<pre><code>get_config(key: str) -&gt; dict | str | None\n</code></pre> <p>Fetch and parse configuration from Consul key-value store.</p> <p>This function automatically prepends the package namespace to the provided key and handles data type conversion based on the stored value format.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Configuration key path relative to the package namespace.  Example keys:  - \"risk_portfolio_config\" -&gt; For risk management settings  - \"data_source/market\" -&gt; For market data source configuration  - \"notification/slack\" -&gt; For Slack notification settings</p> required <p>Returns:</p> Type Description <code>dict | str | None</code> <p>The configuration value with appropriate type conversion:     - dict: For JSON-formatted values (automatically parsed)     - str: For plain text values     - None: If the key doesn't exist</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If unable to connect to Consul</p> <code>ValueError</code> <p>If stored value cannot be parsed</p> <code>TypeError</code> <p>If key is not a string</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = get_config(\"risk_portfolio_config\")\n&gt;&gt;&gt; if config and isinstance(config, dict):\n...     threshold = config.get(\"risk_threshold\", 0.05)\n...     window = config.get(\"window_size\", 252)\n</code></pre>"},{"location":"reference/homelab_airflow_dags/constants/","title":"Constants","text":""},{"location":"reference/homelab_airflow_dags/common_tasks/","title":"Common Tasks","text":""},{"location":"reference/homelab_airflow_dags/common_tasks/oss_operator/","title":"Oss Operator","text":"<p>MinIO Upload Task Module.</p> <p>Simple MinIO upload functionality using Consul configuration. Supports both file-based and memory-based uploads.</p>"},{"location":"reference/homelab_airflow_dags/common_tasks/oss_operator/#homelab_airflow_dags.common_tasks.oss_operator-functions","title":"Functions","text":""},{"location":"reference/homelab_airflow_dags/common_tasks/oss_operator/#homelab_airflow_dags.common_tasks.oss_operator.get_content_type","title":"get_content_type","text":"<pre><code>get_content_type(file_path: str) -&gt; str\n</code></pre> <p>Determine the MIME type of a file based on its file extension.</p> <p>This function uses Python's built-in mimetypes module to guess the MIME type of a file based on its extension. It provides a fallback to a generic binary MIME type for files with unknown or unrecognized extensions.</p> <p>The function is essential for proper file handling in object storage systems like MinIO, as it ensures that files are served with the correct Content-Type header, enabling proper browser handling and preview functionality.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file for which to determine the MIME type.       Only the file extension is used for type detection, so the       file doesn't need to exist at the specified path.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The MIME type string corresponding to the file extension.      Returns \"application/octet-stream\" for unknown file types.</p> Example <p>Common file type detection:</p> <pre><code># Image files\nassert get_content_type(\"image.jpg\") == \"image/jpeg\"\nassert get_content_type(\"photo.png\") == \"image/png\"\n\n# Document files\nassert get_content_type(\"document.pdf\") == \"application/pdf\"\nxlsx_mime = \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\nassert get_content_type(\"spreadsheet.xlsx\") == xlsx_mime\n\n# Text files\nassert get_content_type(\"data.csv\") == \"text/csv\"\nassert get_content_type(\"config.json\") == \"application/json\"\n\n# Unknown extensions\nassert get_content_type(\"file.unknown\") == \"application/octet-stream\"\n</code></pre> Note <ul> <li>The function initializes the mimetypes module if not already done</li> <li>Only the file extension is considered, not the actual file content</li> <li>The detection is case-insensitive for file extensions</li> <li>Binary fallback ensures compatibility with all file types</li> </ul>"},{"location":"reference/homelab_airflow_dags/common_tasks/oss_operator/#homelab_airflow_dags.common_tasks.oss_operator.get_minio_s3_hook","title":"get_minio_s3_hook","text":"<pre><code>get_minio_s3_hook(config_name: str = 'minio_auth_config') -&gt; S3Hook\n</code></pre> <p>Create S3Hook using MinIO configuration from Consul.</p> <p>This function creates an S3Hook by setting up environment variables for the AWS connection, which is a simpler approach that doesn't require database operations.</p> <p>Parameters:</p> Name Type Description Default <code>config_name</code> <code>str</code> <p>Name of the configuration in Consul</p> <code>'minio_auth_config'</code> <p>Returns:</p> Name Type Description <code>S3Hook</code> <code>S3Hook</code> <p>Configured S3Hook instance</p> Example Consul Configuration <pre><code>minio_endpoint: \"http://minio.minio.svc.cluster.local:9000\"\naccess_key: \"your-minio-access-key\"\nsecret_key: \"your-minio-secret-key\"\nregion: \"us-east-1\"\n</code></pre>"},{"location":"reference/homelab_airflow_dags/common_tasks/oss_operator/#homelab_airflow_dags.common_tasks.oss_operator.upload_file","title":"upload_file","text":"<pre><code>upload_file(file_path: str, object_key: str, bucket_name: str = 'airflow', config_name: str = 'minio_auth_config') -&gt; bool\n</code></pre> <p>Upload a file to MinIO object storage with automatic MIME type detection.</p> <p>This function provides the core file upload functionality to MinIO object storage. It handles file validation, MIME type detection, S3Hook configuration, and the actual upload operation with proper error handling and logging.</p> <p>The function automatically detects the file's MIME type based on its extension and sets appropriate metadata for proper file handling in MinIO. It uses the S3Hook from Airflow's Amazon provider to ensure compatibility with S3-compatible storage systems like MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The local file system path to the file that needs to be uploaded.       Must point to an existing file with read permissions.</p> required <code>object_key</code> <code>str</code> <p>The target object key/path within the MinIO bucket. This serves        as the unique identifier for the file within the bucket.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the target MinIO bucket. The bucket must exist         before attempting the upload. Defaults to \"airflow\".</p> <code>'airflow'</code> <code>config_name</code> <code>str</code> <p>The name of the configuration key in Consul that contains         MinIO authentication details. Defaults to \"minio_auth_config\".</p> <code>'minio_auth_config'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file upload completed successfully, False if the upload       failed due to any reason (file not found, connection issues, etc.).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the MinIO configuration is not found in Consul or if required        configuration fields are missing from the Consul configuration.</p> <code>FileNotFoundError</code> <p>If the specified local file does not exist at the given               path or if there are permission issues reading the file.</p> <code>ConnectionError</code> <p>If unable to establish connection to MinIO service or if             authentication fails with the provided credentials.</p> <code>S3UploadFailedError</code> <p>If the upload operation fails due to network issues,                 storage quota exceeded, or other MinIO service errors.</p> Example <p>Basic file upload:</p> <pre><code>success = upload_file(\n    file_path=\"/tmp/data.pkl\",\n    object_key=\"data/processed/result.pkl\",\n    bucket_name=\"analytics\"\n)\nif success:\n    print(\"File uploaded successfully\")\n</code></pre> <p>Upload with custom configuration:</p> <pre><code>success = upload_file(\n    file_path=\"/home/user/report.pdf\",\n    object_key=\"reports/monthly/report.pdf\",\n    bucket_name=\"documents\",\n    config_name=\"custom_minio_config\"\n)\n</code></pre> Note <ul> <li>The function automatically sets Content-Type based on file extension</li> <li>Cache-Control header is set to \"max-age=3600\" for better performance</li> <li>All upload operations use the \"replace=True\" option to overwrite existing files</li> <li>Detailed logging is provided for both success and failure cases</li> </ul>"},{"location":"reference/homelab_airflow_dags/common_tasks/oss_operator/#homelab_airflow_dags.common_tasks.oss_operator.upload_minio","title":"upload_minio","text":"<pre><code>upload_minio(file_path: str, object_key: str, bucket_name: str = 'airflow') -&gt; bool\n</code></pre> <p>Upload a local file to MinIO object storage with automatic retry and MIME type detection.</p> <p>This Airflow task provides a robust file upload mechanism to MinIO object storage with built-in retry logic, automatic MIME type detection, and comprehensive error handling. The task integrates with Consul for dynamic configuration management and uses the S3Hook for reliable file transfer operations.</p> <p>The task performs the following operations: 1. Validates the existence of the local file 2. Retrieves MinIO authentication configuration from Consul 3. Detects the file's MIME type based on its extension 4. Creates an S3Hook connection to MinIO 5. Uploads the file with proper metadata 6. Logs the upload results for monitoring</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The local file system path to the file that needs to be uploaded.       Must be an absolute or relative path to an existing file.</p> required <code>object_key</code> <code>str</code> <p>The target object key/path within the MinIO bucket where the file        will be stored. This acts as the file's identifier in the bucket.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the target MinIO bucket. Defaults to \"airflow\".         The bucket must exist before attempting the upload.</p> <code>'airflow'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file upload was successful, False if the upload failed       due to file not found, connection issues, or other errors.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the MinIO authentication configuration is not found in Consul        or if required configuration fields are missing.</p> <code>FileNotFoundError</code> <p>If the specified local file does not exist at the given path.</p> <code>ConnectionError</code> <p>If unable to establish connection to MinIO service or if             authentication fails with the provided credentials.</p> <code>PermissionError</code> <p>If insufficient permissions to read the local file or write             to the specified MinIO bucket.</p> <code>S3UploadFailedError</code> <p>If the S3/MinIO upload operation fails due to network                 issues, storage quota, or other service-related problems.</p> Note <p>This task is designed for use in Airflow DAGs and requires: - Active Consul service with proper MinIO configuration - Valid \"minio_auth_config\" in Consul containing:   * minio_endpoint: MinIO service endpoint URL   * access_key: MinIO access key for authentication   * secret_key: MinIO secret key for authentication   * region: Storage region (optional, defaults to \"us-east-1\") - Network connectivity to both Consul and MinIO services - Proper file system permissions for reading the source file</p> Example <p>Basic usage in an Airflow DAG:</p> <pre><code>from homelab_airflow_dags.common_tasks.oss_operator import upload_minio\n\n@dag(schedule=\"@daily\")\ndef my_data_pipeline():\n    # Upload processed data file\n    upload_task = upload_minio(\n        file_path=\"/tmp/processed_data.csv\",\n        object_key=\"data/daily/processed_data.csv\",\n        bucket_name=\"data-lake\"\n    )\n</code></pre> <p>Advanced usage with task dependencies:</p> <pre><code>@dag(schedule=\"@daily\")\ndef advanced_pipeline():\n    # Process data first\n    process_task = process_data()\n\n    # Then upload the result\n    upload_task = upload_minio(\n        file_path=\"{{ ti.xcom_pull(task_ids='process_data')['output_file'] }}\",\n        object_key=\"results/{{ ds }}/output.pkl\",\n        bucket_name=\"analytics\"\n    )\n\n    process_task &gt;&gt; upload_task\n</code></pre> <p>Consul configuration example for \"minio_auth_config\":</p> <pre><code>{\n    \"minio_endpoint\": \"http://minio.homelab.local:9000\",\n    \"access_key\": \"minioadmin\",\n    \"secret_key\": \"minioadmin123\",\n    \"region\": \"us-east-1\"\n}\n</code></pre> See Also <p>upload_file: The underlying function that performs the actual upload operation. get_minio_s3_hook: Function that creates the S3Hook with MinIO configuration. get_content_type: Function that determines the MIME type of files.</p>"},{"location":"reference/homelab_airflow_dags/dags/","title":"Dags","text":""},{"location":"reference/homelab_airflow_dags/dags/ibkr_account_snapshot/","title":"Ibkr Account Snapshot","text":""},{"location":"reference/homelab_airflow_dags/dags/ibkr_account_snapshot/#homelab_airflow_dags.dags.ibkr_account_snapshot-functions","title":"Functions","text":""},{"location":"reference/homelab_airflow_dags/dags/ibkr_account_snapshot/#homelab_airflow_dags.dags.ibkr_account_snapshot.ibkr_account_snapshot_dag","title":"ibkr_account_snapshot_dag","text":"<pre><code>ibkr_account_snapshot_dag()\n</code></pre> <p>IBKR\u8d26\u6237\u5feb\u7167\u6570\u636e\u91c7\u96c6DAG.</p>"}]}